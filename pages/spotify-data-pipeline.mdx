import { Callout, Tabs, Cards, Steps, FileTree } from 'nextra/components'
import { Screenshot } from 'components/screenshot'

# Spotify data pipeline

import img_featured from '../public/assets/spotify-data-pipeline/featured.jpg'

<Screenshot src={img_featured} imageName="spotify-data-pipeline/featured.jpg" />

---

## 1. Introduction

Trong project n√†y m√¨nh s·∫Ω h∆∞·ªõng d·∫´n x√¢y d·ª±ng m·ªôt data pipeline c∆° b·∫£n theo m√¥ h√¨nh ELT (extract - load - transform), s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ spotify ƒë·ªÉ ph√¢n t√≠ch xu h∆∞·ªõng nghe nh·∫°c.

Project n√†y ho√†n th√†nh d·ª±a tr√™n ki·∫øn th·ª©c ƒë√£ h·ªçc ƒë∆∞·ª£c t·ª´ kh√≥a Fundamental Data Engineering c·ªßa AIDE. Xin g·ª≠i l·ªùi c·∫£m ∆°n ƒë·∫∑c bi·ªát t·ªõi th·∫ßy Nguy·ªÖn Thanh B√¨nh, anh √îng Xu√¢n H·ªìng v√† anh H√πng L√™.

<Cards.Card arrow title="Project Link" href="https://github.com/lelouvincx/spotify-data-pipeline-ingestion" />

## 2. Objective

M·ª•c ti√™u c·ªßa project n√†y l√† x√¢y d·ª±ng m·ªôt data pipeline ƒë·ªÉ ƒë∆∞a d·ªØ li·ªáu c·ªßa b·∫£ng `spotify_tracks` t·ª´ mySQL v√† `my_tracks` t·ª´ API c·ªßa Spotify th√†nh dashboard ƒë·ªÉ ph√¢n t√≠ch. C√°c b·∫£ng ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi `spotify_albums` v√† `spotify_artists` nh∆∞ m√¥ t·∫£ d∆∞·ªõi ƒë√¢y:

1. `spotify_tracks`: OLTP table ch·ª©a th√¥ng tin b√†i h√°t t·ª´ spotify
2. `my_tracks`: l·ªãch s·ª≠ stream nh·∫°c c·ªßa b·∫£n th√¢n, l·∫•y schema gi·ªëng `spotify_tracks`
3. `spotify_albums`: ch·ª©a th√¥ng tin albums t·ª´ dataset
4. `spotify_artists`: th√¥ng tin ngh·ªá sƒ© t·ª´ dataset

import img_dataset from '../public/assets/spotify-data-pipeline/dataset.png'

<Screenshot src={img_dataset} />

<Callout icon="üé®">
Chi ti·∫øt h∆°n xem ·ªü: [Exploratory Data Analysis](https://github.com/lelouvincx/spotify-data-pipeline-ingestion/blob/main/EDA.ipynb)
</Callout>

## 3. Design

### 3.1 Pipeline design

Ch√∫ng ta s·ª≠ d·ª•ng m√°y ·∫£o `AWS EC2` ƒë·ªÉ t√≠nh to√°n v√† [dagster](https://dagster.io/) ƒë·ªÉ orchestrate tasks.

1. D·ªØ li·ªáu `Spotify` ƒë∆∞·ª£c download t·ª´ Kaggle d∆∞·ªõi d·∫°ng CSV, sau ƒë√≥ import v√†o `MySQL` m√¥ ph·ªèng l√†m d·ªØ li·ªáu doanh nghi·ªáp.
2. D·ªØ li·ªáu streaming history c·ªßa b·∫£n th√¢n ƒë∆∞·ª£c extract t·ª´ Spotify API.
3. Extract 2 ngu·ªìn d·ªØ li·ªáu tr√™n b·∫±ng Pandas ƒë·ªÉ preprocessing (optimize size consumed).
4. Load v√†o `AWS S3`, t·ª´ ƒë√≥ load ti·∫øp v√†o data warehouse `PostgreSQL` ƒë·ªÉ l√†m analytics.
5. Transform d·ªØ li·ªáu b·∫±ng `dbt` tr√™n n·ªÅn `PostgreSQL`
6. Tr·ª±c quan h√≥a d·ªØ li·ªáu b·∫±ng `Metabase`

import img_pipeline_design from '../public/assets/spotify-data-pipeline/pipeline_design.png'

<Screenshot src={img_pipeline_design} />

### 3.2 Data lake structure

Ch√∫ng ta s·ª≠ d·ª•ng `AWS S3` l√†m data lake. M·ªçi d·ªØ li·ªáu tr∆∞·ªõc h·∫øt s·∫Ω ƒë∆∞·ª£c ch·ª©a ·ªü ƒë√¢y.
Trong project n√†y, ta ch·ªâ c·∫ßn 1 bucket v·ªõi nhi·ªÅu th∆∞ m·ª•c.

1. **Bronze**: L∆∞u d·ªØ li·ªáu th√¥ m·ªõi l·∫•y v·ªÅ. Ch√∫ng l√† step 1, 2, 3 trong pipeline design.
2. **Silver**: L∆∞u d·ªØ li·ªáu ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω. Ch√∫ng l√† step 4 trong pipeline design.
3. **Gold**: L∆∞u d·ªØ li·ªáu s·∫°ch khi transform b·∫±ng dbt (step 5).

<FileTree>
  <FileTree.Folder name="bronze" defaultOpen>
    <FileTree.Folder name="personal" defaultOpen>
      <FileTree.File name="my_tracks" />
    </FileTree.Folder>
  </FileTree.Folder>
  <FileTree.Folder name="silver" defaultOpen>
    <FileTree.Folder name="personal" defaultOpen>
      <FileTree.File name="my_tracks" />
    </FileTree.Folder>
    <FileTree.Folder name="spotify" defaultOpen>
      <FileTree.File name="spotify_albums" />
      <FileTree.File name="spotify_artists" />
      <FileTree.File name="spotify_tracks" />
    </FileTree.Folder>
  </FileTree.Folder>
  <FileTree.Folder name="gold" defaultOpen>
    <FileTree.Folder name="personal" defaultOpen>
      <FileTree.File name="my_tracks" />
    </FileTree.Folder>
    <FileTree.Folder name="spotify" defaultOpen>
      <FileTree.File name="spotify_albums" />
      <FileTree.File name="spotify_artists" />
      <FileTree.File name="spotify_tracks" />
    </FileTree.Folder>
  </FileTree.Folder>
</FileTree>

### 3.3 Directory tree

- **docker-compose**: compose c√°c container ch·∫°y trong docker
- **EDA**: kh√°m ph√° dataset v√† profiling
- **.gitignore**: gi√∫p git kh√¥ng track file (nh∆∞ env, cache, ...)
- **.gitlab-ci**: config qu√° tr√¨nh CI tr√™n gitlab
- **Makefile**: thu g·ªçn c√¢u l·ªánh
- **requirements.txt**: packages python c·∫ßn thi·∫øt v√† thi·∫øt l·∫≠p virtualenv
- Folder **dagser_home** ch·ª©a dagster.yaml ƒë·ªÉ config th√†nh ph·∫ßn dagster c√≤n workspace.yaml ƒë·ªÉ ch·ªâ ƒë·ªãnh dagster ch·∫°y host elt_pipeline
- Folder **dockers** ch·ª©a file config c√°c container: dagster v√† jupyter
- Folder **load_dataset** ch·ª©a c√°c file d√πng ƒë·ªÉ load d·ªØ li·ªáu ban ƒë·∫ßu v√†o mySQL
- Folder **terraform** ƒë·ªÉ kh·ªüi t·∫°o v√† config server tr√™n AWS

<FileTree>
  <FileTree.File name="docker-compose.yml" />
  <FileTree.File name="EDA.ipynb" />
  <FileTree.File name=".env" />
  <FileTree.File name=".gitignore" />
  <FileTree.File name=".gitlab-ci.yml" />
  <FileTree.File name="Makefile" />
  <FileTree.File name="README.md" />
  <FileTree.File name="requirements.txt" />
  <FileTree.Folder name="elt_pipeline" />
  <FileTree.Folder name="cicd" />
  <FileTree.Folder name="dagster_home" >
    <FileTree.File name="dagster.yaml" />
    <FileTree.File name="workspace.yaml" />
  </FileTree.Folder>
  <FileTree.Folder name="dataset" >
    <FileTree.File name="spotify_tracks.csv" />
    <FileTree.File name="spotify_artists" />
    <FileTree.File name="spotify_albums" />
  </FileTree.Folder>
  <FileTree.Folder name="docker" >
    <FileTree.Folder name="jupyter" >
      <FileTree.File name="Dockerfile" />
      <FileTree.File name="requirements.txt" />
    </FileTree.Folder>
    <FileTree.Folder name="dagster" >
      <FileTree.File name="Dockerfile" />
      <FileTree.File name="requirements.txt" />
    </FileTree.Folder>
  </FileTree.Folder>
  <FileTree.Folder name="load_dataset" >
    <FileTree.File name="mysql_datasource.sql" />
    <FileTree.File name="mysql_load_csv.sql" />
    <FileTree.File name="mysql_set_foreign_key.csv" />
  </FileTree.Folder>
  <FileTree.Folder name="terraform" />
</FileTree>

<Callout icon="üé®">
Chi ti·∫øt c√¢y th∆∞ m·ª•c xem ·ªü: [directory tree](https://github.com/lelouvincx/spotify-data-pipeline-ingestion/blob/main/tree.txt)
</Callout>

## 4. Setup

<Steps>
### 4.1 Prequisites

ƒê·ªÉ s·ª≠ d·ª•ng pipeline n√†y, download & install nh·ªØng ph·∫ßn m·ªÅm sau:

1. [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)
2. [T√†i kho·∫£n gitlab](https://gitlab.com/)
3. [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli)
4. [T√†i kho·∫£n AWS](https://aws.amazon.com/)
5. [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html) v√† [configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
6. [Docker](https://docs.docker.com/engine/install/) √≠t nh·∫•t 4GB RAM v√† [Docker Compose](https://docs.docker.com/compose/install/) √≠t nh·∫•t v1.29.2

N·∫øu d√πng Windows, setup th√™m WSL v√† m·ªôt m√°y ·∫£o local Ubuntu, c√†i ƒë·∫∑t nh·ªØng th·ª© tr√™n cho ubuntu.

### 4.2 Setup data infrastructure local

Clone repository:

```bash
git clone https://gitlab.com/lelouvincx/fde01_project_fde220103_dinhminhchinh.git
mv fde01_project_fde220103_dinhminhchinh project
cd project

# Create env file
touch env
cp env.template env
```

ƒêi·ªÅn c√°c bi·∫øn m√¥i tr∆∞·ªùng v√†o file env.

Ch·∫°y c√°c l·ªánh sau ƒë·ªÉ setup infra d∆∞·ªõi local:

```bash
# Setup python packages
make install

# Build docker images
make build

# Run locally
make up

# Check running containers
docker ps

# Check code quality
make check
make lint

# Use black to reformat if any tests failed, then try again
black ./elt_pipeline

# Test coverage
make test
```

<Callout type="info">
L√∫c n√†y s·∫Ω c√≥ 7 services sau ƒëang ch·∫°y:

import img_services from '../public/assets/spotify-data-pipeline/services.png'

<Screenshot src={img_services} />
</Callout>

B√¢y gi·ªù ch√∫ng ta import dataset spotify (d·∫°ng csv) v√†o mySQL:

```bash
# Enter mysql cli
make to_mysql
```

```sql
SET GLOBAL local_infile=true;
-- Check if local_infile is turned on
SHOW VARIABLES LIKE "local_infile";
exit
```

Source t·ª´ng file theo th·ª© t·ª±:

```bash
# Create tables with schema
make mysql_create
# Load csv into created tables
make mysql_load
# Set their foreign keys
make mysql_set_foreign_key
```

Kh·ªüi t·∫°o schema v√† table trong PostgreSQL:

```bash
# Enter psql cli
make psql_create
```

Testing:

```bash
# Test utils
python3 -m pytest -vv --cov=utils elt_pipeline/tests/utils
# Test ops
python3 -m pytest -vv --cov=ops elt_pipeline/tests/ops
```

Truy c·∫≠p giao di·ªán c·ªßa pipeline b·∫±ng dagit: https://localhost:3001

### 4.3 Setup data infrastructure on AWS

#### Terraform

Ch√∫ng ta d√πng terraform l√†m IaC (Infrastructure as Code) ƒë·ªÉ setup h·∫° t·∫ßng tr√™n AWS (nh·ªõ [c·∫•p credential key cho AWS](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/aws-build) nh√©)

```bash
cd terraform

# Initialize infra
make tf-init

# Checkout planned infra
make tf-plan

# Build up infra
make tf-up
```

ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ setup xong. Ch√∫ng ta l√™n [Amazon Web Services](aws.amazon.com)

import img_ec2 from '../public/assets/spotify-data-pipeline/ec2.png'
import img_s3 from '../public/assets/spotify-data-pipeline/s3.png'

- Trong EC2 s·∫Ω th·∫•y 1 m√°y ·∫£o t√™n project-spotify-EC2
<Screenshot src={img_ec2} />

- Trong S3 th·∫•y 1 bucket t√™n project-spotify-bucket
<Screenshot src={img_s3} />

Sau khi project-spotify-EC2 hi·ªán ƒë√£ pass h·∫øt status th√¨ ch√∫ng ta ƒë√£ setup th√†nh c√¥ng.

#### Truy c·∫≠p v√†o EC2 ƒë·ªÉ ho√†n t·∫•t setup

```bash
# Connect to EC2 from local terminal
make ssh-ec2
# Generate new ssh key for gitlab
ssh-keygen
# Then press Enter until done
cat ~/.ssh/id_rsa.pub
```

- Copy ƒëo·∫°n m√£ SSH
- V√†o gitlab, ph√≠a tr√™n g√≥c ph·∫£i c√≥ h√¨nh avatar -> Preferences -> SSH Keys -> paste key v·ª´a copy v√†o -> ƒë·∫∑t t√™n l√† `project-spotify-vm` -> Add key
- V√†o terminal c·ªßa EC2 (v·ª´a connect l√∫c n√£y), clone v·ªÅ b·∫±ng SSH
- L·∫∑p l·∫°i b∆∞·ªõc setup infra local ƒë√£ tr√¨nh b√†y ·ªü ph·∫ßn tr√™n

</Steps>

## 5. Detailed code walkthrough

ELT pipeline g·ªìm 2 job ch·∫°y 2 t√°c v·ª• ƒë·ªôc l·∫≠p: EL data t·ª´ MySQL v√† EL data t·ª´ API nh∆∞ng nh√¨n chung ch√∫ng c√≥ c·∫•u tr√∫c gi·ªëng nhau. C·ª• th·ªÉ:

1. `extract_data_from_{mysql/api}`: L·∫•y data t·ª´ MySQL ho·∫∑c api (th√¥ng qua `access token`) v√† l∆∞u t·∫°m d∆∞·ªõi d·∫°ng `pandas.DataFrame`. T√πy theo chi·∫øn l∆∞·ª£c ingest data (full load/incremental by partition/incremental by watermark) m√† c√≥ c√°ch gi·∫£i quy·∫øt ph√π h·ª£p.
2. `load_data_to_s3`: Ti·ªÅn x·ª≠ l√Ω `data types` cho `DataFrame` t·ª´ upstream v√† load v√†o S3 d∆∞·ªõi d·∫°ng parquet.
3. `load_data_to_psql`: Extract data d·∫°ng parquet trong S3 th√†nh `pandas.DataFrame` v√† load v√†o PostgreSQL. ƒê·ªÉ d·ªØ li·ªáu ƒë∆∞·ª£c to√†n v·∫πn (kh√¥ng b·ªã crash, l·ªói ƒë∆∞·ªùng truy·ªÅn) trong qu√° tr√¨nh crash, ta t·∫°o `TEMP TABLE` v√† load v√†o ƒë√≥ tr∆∞·ªõc.
4. `validate_{mssql2psql/api2psql}_ingestion`: Th·∫©m ƒë·ªãnh 3 step tr√™n ƒë√£ ƒë∆∞·ª£c EL th√†nh c√¥ng hay ch∆∞a
5. `trigger_dbt_spotify`: Sensor ƒë·ªÉ trigger `dbt` nh·∫±m transform data.

import job_mssql2psql_ingestion from '../public/assets/spotify-data-pipeline/mssql2psql_ingestion.png'

<Screenshot src={job_mssql2psql_ingestion} />

### 5.1 Extract

L·∫•y data t·ª´ MySQL ho·∫∑c api (th√¥ng qua `access token`) v√† l∆∞u t·∫°m d∆∞·ªõi d·∫°ng `pandas.DataFrame`. T√πy theo chi·∫øn l∆∞·ª£c ingest data (full load/incremental by partition/incremental by watermark) m√† c√≥ c√°ch gi·∫£i quy·∫øt ph√π h·ª£p.

Ta ƒë·ªãnh nghƒ©a ph∆∞∆°ng th·ª©c extract data c·ªßa mysql v√† api trong th∆∞ m·ª•c utils.

```python showLineNumbers filename="utils/mysql_loader/extract"
def extract_data(self, sql: str) -> pd.DataFrame:
    pd_data = None
    with self.get_db_connection() as db_conn:
        pd_data = pd.read_sql(sql, db_conn)
    return pd_data
```

```python showLineNumbers filename="utils/api_loader/get_recently" {10}
def get_recently(self, number: int, token: str) -> (int, dict):
    headers = {
        "Accept": "application/json",
        "Content-Type": "application/json",
        "Authorization": f"Bearer " + token,
    }
    params = [("limit", number),]
    try:
        response = requests.get(
            "https://api.spotify.com/v1/me/player/recently-played",
            headers=headers,
            params=params,
            timeout=10,
        )
        return (response.status_code, response.json())
    except:
        return None
```

```python showLineNumbers filename="utils/api_loader/extract"
def extract_data(self, token: str) -> pd.DataFrame:
    (code, content) = self.get_recently(50, token)
    my_tracks = {
        "album_id": [], "artists_id": [], "track_id": [], "track_unique_id": [], "name": [], "popularity": [], "type": [], "duration_ms": [], "played_at": [], "danceability": [], "energy": [], "track_key": [], "loudness": [], "mode": [], "speechiness": [], "acousticness": [], "instrumentalness": [], "liveness": [], "valence": [], "tempo": [],
    }

    items = content.get("items", [])
    for item in items:
        # Take album_id, artists_id, track_id, name, popularity, type, duration_ms
        played_at = item.get("played_at", [])
        track = item.get("track", [])
        album = track.get("album", [])
        album_id = album.get("id", [])
        artists = track.get("artists", [])
        artists_id = []
        for artist in artists:
            artists_id.append(artist.get("id", []))
        track_id = track.get("id", [])
        name = track.get("name", [])
        popularity = track.get("popularity", [])
        type = track.get("type", [])
        duration_ms = track.get("duration_ms", [])

        # Take features
        features = self.get_features(track_id, token)
        danceability = features.get("danceability", [])
        energy = features.get("energy", [])
        track_key = features.get("key", [])
        loudness = features.get("loudness", [])
        mode = features.get("mode", [])
        speechiness = features.get("speechiness", [])
        acousticness = features.get("acousticness", [])
        instrumentalness = features.get("instrumentalness", [])
        liveness = features.get("liveness", [])
        valence = features.get("valence", [])
        tempo = features.get("tempo", [])

        # Extract row into dict
        my_tracks["album_id"].append(album_id)
        my_tracks["artists_id"].append(artists_id)
        my_tracks["track_id"].append(track_id)
        my_tracks["track_unique_id"].append(track_id + played_at)
        my_tracks["name"].append(name)
        my_tracks["popularity"].append(popularity)
        my_tracks["type"].append(type)
        my_tracks["duration_ms"].append(duration_ms)
        my_tracks["played_at"].append(played_at[:10])
        my_tracks["danceability"].append(danceability)
        my_tracks["energy"].append(energy)
        my_tracks["track_key"].append(track_key)
        my_tracks["loudness"].append(loudness)
        my_tracks["mode"].append(mode)
        my_tracks["speechiness"].append(speechiness)
        my_tracks["acousticness"].append(acousticness)
        my_tracks["instrumentalness"].append(instrumentalness)
        my_tracks["liveness"].append(liveness)
        my_tracks["valence"].append(valence)
        my_tracks["tempo"].append(tempo)

    pd_data = pd.DataFrame(my_tracks)
    return pd_data
```

Gi·ªù l√† l√∫c extract data.

```python showLineNumbers filename="extract_data_from_mysql" {10-14}
def extract_data_from_mysql(context, run_config):
    updated_at = context.op_config.get("updated_at")
    context.log.info(f"Updated at: {updated_at}")
    if updated_at is None or updated_at == "":
        context.log.info("Nothing to do!")
        return None
    context.log.info(f"Op extracts data from MySQL at {updated_at}")

    # Choose extract strategy (default: full load)
    sql_stm = f"""
        SELECT *
        FROM {run_config.get('src_tbl')}
        WHERE 1=1
    """
    if run_config.get("strategy") == "incremental_by_partition":
        if updated_at != "init_dump":
            sql_stm += f""" AND CAST({run_config.get('partition')} AS DATE) = '{updated_at}' """

    if run_config.get("strategy") == "incremental_by_watermark":
        data_loader = get_data_loader(
            run_config.get("db_provider"), run_config.get("target_db_params")
        )
        watermark = data_loader.get_watermark(
            f"{run_config.get('target_schema')}.{run_config.get('target_tbl')}",
            run_config.get("watermark"),
        )
        watermark = (
            updated_at if watermark is None or watermark > updated_at else watermark
        )
        if updated_at != "init_dump":
            sql_stm += f""" AND {run_config.get('watermark')} >= '{watermark}' """

    context.log.info(f"Extracting with SQL: {sql_stm}")
    db_loader = MysqlLoader(run_config.get("src_db_params"))
    pd_data = db_loader.extract_data(sql_stm)
    context.log.info(f"Data extracted successfully with shape: {pd_data.shape}")

    # Update params
    run_config.update(
        {
            "updated_at": updated_at,
            "data": pd_data,
            "s3_path": f"bronze/{run_config.get('data_source')}/{run_config.get('ls_target').get('target_tbl')}",
            "load_dtypes": run_config.get("load_dtypes"),
        }
    )

    return run_config
```

```python showLineNumbers filename="extract_data_from_api"
def extract_data_from_api(context, run_config):
    updated_at = context.op_config.get("updated_at")
    context.log.info(f"Updated at: {updated_at}")
    if updated_at is None or updated_at == "":
        context.log.info("Nothing to do!")
        return None
    context.log.info(f"Op extracts data from API at {updated_at}")

    # Extract strategy (only support incremental_by_partition)
    context.log.info(f"Extracting on date: {updated_at}")
    api_loader = ApiLoader(run_config.get("src_api_params"))
    token = api_loader.get_api_token()
    pd_data = api_loader.extract_data(token)
    index_played_at = pd_data[pd_data["played_at"] != updated_at].index  # Drop data
    pd_data.drop(index_played_at, inplace=True)
    context.log.info(
        f"Data loaded and filtered successfully with shape: {pd_data.shape}"
    )

    run_config.update(
        {
            "updated_at": updated_at,
            "data": pd_data,
            "s3_path": f"bronze/{run_config.get('data_source')}/{run_config.get('ls_target').get('target_tbl')}",
            "load_dtypes": run_config.get("load_dtypes"),
        }
    )

    return run_config
```

### 5.2 Load

Ti·ªÅn x·ª≠ l√Ω data types cho DataFrame t·ª´ upstream v√† load v√†o S3 d∆∞·ªõi d·∫°ng parquet.

```python showLineNumbers filename="load_data_to_s3" {43-46}
def load_data_to_s3(context, upstream):
    if upstream is None:
        return None

    updated_at = upstream.get("updated_at")
    s3_bucket = os.getenv("DATALAKE_BUCKET")
    if type(updated_at) == list:
        updated_at = max(updated_at)
    s3_file = f"s3://{s3_bucket}/{upstream.get('s3_path')}/updated_at={updated_at}"
    context.log.info(f"Loading data to S3: {s3_file}")

    # Load data to S3
    pd_data = upstream.get("data")

    # Preprocess data
    load_dtypes = upstream.get("load_dtypes")
    try:
        for col, data_type in load_dtypes.items():
            if data_type == "str":
                pd_data[col] = pd_data[col].fillna("")
                pd_data[col] = pd_data[col].astype(str)
                pd_data[col] = pd_data[col].str.strip()
                pd_data[col] = pd_data[col].str.rstrip()
                pd_data[col] = pd_data[col].str.replace("'", "")
                pd_data[col] = pd_data[col].str.replace('"', "")
                pd_data[col] = pd_data[col].str.replace(r"\n", "", regex=True)
            elif data_type == "int":
                cur_bit = np.log2(pd_data[col].max())
                if cur_bit > 32:
                    pd_data[col] = pd_data[col].astype({col: "int64"})
                elif cur_bit > 16:
                    pd_data[col] = pd_data[col].astype({col: "int32"})
                elif cur_bit > 8:
                    pd_data[col] = pd_data[col].astype({col: "int16"})
                else:
                    pd_data[col] = pd_data[col].astype({col: "int8"})
            elif data_type == "float":
                pd_data[col] = pd_data[col].astype({col: "float32"})
        context.log.info(f"Data preprocessed successfully")
    except Exception as e:
        context.log.info(f"Exception: {e}")

    # Write parquet object to S3
    pa_data = pa.Table.from_pandas(df=pd_data, preserve_index=False)
    pq.write_table(pa_data, s3_file)
    context.log.info("Data loaded successfully to S3")

    # Update stream
    upstream.update({"s3_bucket": s3_bucket, "s3_file": s3_file})

    return upstream
```

```python showLineNumbers filename="load_data_to_psql" {15-19}
def load_data_to_psql(context, upstream):
    if upstream is None:
        return None

    # Load data to target
    context.log.info("Loading data to postgreSQL")
    context.log.info(f"Extracting data from {upstream.get('s3_file')}")
    pd_stag = pd.read_parquet(upstream.get("s3_file"))
    context.log.info(f"Extracted data shape: {pd_stag.shape}")

    if len(pd_stag) == 0:
        context.log.info("No data to upload!")
        return "No data to upload!"

    # Execute
    db_loader = PsqlLoader(upstream.get("target_db_params"))
    result = db_loader.load_data(pd_stag, upstream)
    context.log.info(f"Batch inserted status: {result}")
    return result
```

### 5.3 Transform

Do dataset h·∫ßu h·∫øt ƒë√£ ƒë∆∞·ª£c clean. Ch√∫ng ta ch·ªçn c√°c column c·∫ßn thi·∫øt cho vi·ªác visualization.

```sql showLineNumbers filename="cleaned_my_tracks.sql" {5-7}
{{ config(materialized='table') }}

with my_unique_tracks AS (

  select *
  from spotify.my_tracks
  group by track_unique_id

)

select * from my_unique_tracks
```

```sql showLineNumbers filename="cleaned_spotify_tracks.sql" {5-7}
{{ config(materialized='table') }}

with unique_tracks AS (

  select acousticness, album_id, artists_id, country, danceability, duration_ms, energy, track_id, instrumentalness, track_key, liveness, loudness, mode, name, popularity, speechiness, tempo, valence
  from spotify.spotify_tracks
  group by track_id

)

select * from unique_tracks
```

```sql showLineNumbers filename="cleaned_spotify_artists.sql" {5-7}
{{ config(materialized='table') }}

with unique_artists AS (

  select artist_popularity, followers, genres, artist_id, name, track_id
  from spotify.spotify_artists
  group by artist_id

)

select * from unique_artists
```

```sql showLineNumbers filename="cleaned_spotify_albums.sql" {5-7}
{{ config(materialized='table') }}

with unique_albums AS (

  select album_id, artist_id, album_type, name, release_date, release_date_precision, total_tracks, track_id
  from spotify.spotify_albums
  group by album_id

)

select * from unique_albums
```

### 5.4 Check results

Sau khi transform th√†nh c√¥ng, m·ªü `Dbeaver` l√™n v√† ch√∫ng ta th·∫•y c√°c table ƒë√£ s·∫µn s√†ng ƒë·ªÉ analytics.

## 6. Tear down infrastructure

D·ª° b·ªè infra sau khi xong vi·ªác (*th·ª±c hi·ªán d∆∞·ªõi m√°y local*):

```bash
# Tear down containers
make down

# Tear down AWS
cd terraform
make tf-down
```

<Callout type="warning">
**Note:** L√™n AWS ki·ªÉm tra l·∫°i c√°c services sau ƒë√£ d·ª´ng v√† b·ªã x√≥a ch∆∞a (n·∫øu kh√¥ng mu·ªën m·∫•t ti·ªÅn oan nh∆∞ m√¨nh): EC2, S3.
</Callout>

## 7. Design considerations

Sau khi deploy th√†nh c√¥ng pipeline, gi·ªù l√† l√∫c ƒë√°nh gi√° project.

1. **T·ªëc ƒë·ªô**: T·ªëc ƒë·ªô extract data kh√° ch·∫≠m (v√¨ load v√†o `pandas.DataFrame` 2 l·∫ßn). M·ªôt s·ªë gi·∫£i ph√°p thay th·∫ø: [polars](https://pola-rs.github.io/polars-book/user-guide/introduction.html), `json`, ...
2. **K√≠ch th∆∞·ªõc**: Chuy·ªán g√¨ s·∫Ω x·∫£y ra khi data l·ªõn l√™n g·∫•p 10x, 100x, 1000x? L√∫c ƒë·∫•y ta c·∫ßn xem x√©t c√°c gi·∫£i ph√°p gi√∫p l∆∞u tr·ªØ big data, thay ƒë·ªïi data warehouse th√†nh Amazon RDS, Google BigQuery, ...
3. **M√¥i tr∆∞·ªùng ph√°t tri·ªÉn**: Khi project c√≥ th√™m nhi·ªÅu ng∆∞·ªùi c√πng s·ª≠ d·ª•ng l√† c≈©ng l√† l√∫c ph√¢n chia m√¥i tr∆∞·ªùng th√†nh testing, staging, production.

## 8. Further actions

1. **TƒÉng l∆∞·ª£ng data**: T√≠ch h·ª£p nhi·ªÅu data h∆°n t·ª´ Spotify API: Khi ingest b√†i h√°t m·ªõi, ingest lu√¥n th√¥ng tin v·ªÅ artist, album, t·∫°o th√†nh h·ªá sinh th√°i b√†i h√°t ƒë·∫ßy ƒë·ªß.
2. **Stream ingestion**: D√πng m·ªôt tech stack kh√°c cho job API theo h∆∞·ªõng streaming. H·ªá th·ªëng s·∫Ω listen m·ªói l·∫ßn nghe xong b√†i h√°t l√† t·ª± ƒë·ªông c·∫≠p nh·∫≠t v√†o pipeline.
3. **My wrap-up**: T·ª± th·ª±c h√†nh ph√¢n t√≠ch d·ªØ li·ªáu nh∆∞ t√≠nh nƒÉng wrap-up c·ªßa spotify.
4. **Recommender system**: Th·ª±c h√†nh l√†m m·ªôt h·ªá th·ªëng g·ª£i √Ω d·ª±a tr√™n nh·ªØng b√†i ƒë√£ nghe.
